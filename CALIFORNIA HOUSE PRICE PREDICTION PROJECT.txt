# CALIFORNIA HOUSE PRICE PREDICTION PROJECT
## Complete Code Documentation for Tutorial Generation

This document contains all the code files for a machine learning web application that predicts California house prices. The project consists of three main code files that work together to create a complete end-to-end solution.

---
---

## FILE 1: index.html
## Purpose: User Interface (Front-End)

This is the HTML file that creates the web form where users input housing details and see price predictions.

### What This File Does:
- Creates a web form with input fields for all 9 features needed by the model
- Organizes inputs into logical sections (Location, Housing Specs, Demographics)
- Displays prediction results in a green box when available
- Shows error messages in a red box if something goes wrong
- Uses Jinja2 templating (Flask's template engine) to display dynamic content

### Key Features:
- Responsive design with maximum width for readability
- Default values pre-filled to help users understand expected input format
- Dropdown menu for ocean proximity (categorical variable)
- Form validation with required fields and min/max constraints
- Sends data to the /predict endpoint via POST method

---

### COMPLETE CODE:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>California Housing Price Predictor</title>
</head>
<body style="font-family: sans-serif; max-width: 600px; margin: 2rem auto; padding: 0 1rem;">

    <h1>House Price Prediction</h1>
    <p>Enter the details of the block group to predict the median house value.</p>
    <hr>

    {% if error %}
    <div style="color: red; border: 1px solid red; padding: 10px; margin-bottom: 20px; background-color: #ffeeee;">
        <strong>Error:</strong> {{ error }}
    </div>
    {% endif %}

    {% if prediction_text %}
    <div style="color: green; border: 1px solid green; padding: 20px; margin-bottom: 20px; background-color: #eeffee;">
        <h2>Estimated Price: {{ prediction_text }}</h2>
    </div>
    {% endif %}

    <form action="/predict" method="POST">
        
        <fieldset>
            <legend>Location Details</legend>
            <label for="longitude">Longitude (e.g., -122.23):</label><br>
            <input type="number" step="any" name="longitude" required value="{{ input_data['longitude'] if input_data else '-122.23' }}"><br><br>

            <label for="latitude">Latitude (e.g., 37.88):</label><br>
            <input type="number" step="any" name="latitude" required value="{{ input_data['latitude'] if input_data else '37.88' }}"><br><br>

            <label for="ocean_proximity">Ocean Proximity:</label><br>
            <select name="ocean_proximity" required>
                <option value="<1H OCEAN">Less than 1H to Ocean</option>
                <option value="INLAND">Inland</option>
                <option value="NEAR OCEAN">Near Ocean</option>
                <option value="NEAR BAY">Near Bay</option>
                <option value="ISLAND">Island</option>
            </select>
        </fieldset>
        <br>

        <fieldset>
            <legend>Housing Specs (Block Averages)</legend>
            <label for="housing_median_age">Median House Age (Years):</label><br>
            <input type="number" step="any" name="housing_median_age" required min="1" max="100" value="{{ input_data['housing_median_age'] if input_data else '41' }}"><br><br>

            <label for="total_rooms">Total Rooms:</label><br>
            <input type="number" step="any" name="total_rooms" required min="1" value="{{ input_data['total_rooms'] if input_data else '880' }}"><br><br>

            <label for="total_bedrooms">Total Bedrooms:</label><br>
            <input type="number" step="any" name="total_bedrooms" required min="1" value="{{ input_data['total_bedrooms'] if input_data else '129' }}"><br><br>
        </fieldset>
        <br>

        <fieldset>
            <legend>Demographics</legend>
            <label for="population">Population:</label><br>
            <input type="number" step="any" name="population" required min="1" value="{{ input_data['population'] if input_data else '322' }}"><br><br>

            <label for="households">Households:</label><br>
            <input type="number" step="any" name="households" required min="1" value="{{ input_data['households'] if input_data else '126' }}"><br><br>

            <label for="median_income">Median Income (Tens of Thousands, e.g., 8.3 = $83k):</label><br>
            <input type="number" step="any" name="median_income" required min="0" value="{{ input_data['median_income'] if input_data else '8.32' }}">
        </fieldset>
        <br>

        <button type="submit" style="padding: 10px 20px; font-size: 16px; cursor: pointer;">Predict Price</button>
        <a href="/" style="margin-left: 10px;">Clear Form</a>
    </form>

</body>
</html>
```

### Important Notes About This HTML File:
- The curly braces with percent signs {% %} are Jinja2 template syntax used by Flask
- The double curly braces {{ }} display variables passed from the Flask backend
- Input names must EXACTLY match the feature names expected by the machine learning model
- The form posts data to "/predict" which is handled by the Flask app
- Default values are realistic examples from the California Housing dataset

---
---

## FILE 2: train_model.py
## Purpose: Model Training (Machine Learning Pipeline Creation)

This Python script downloads data, trains the machine learning model, and saves it for later use by the web application.

### What This File Does:
1. Downloads the California Housing dataset from GitHub
2. Separates features (X) from the target variable (y)
3. Defines preprocessing steps for numeric and categorical data
4. Creates a complete pipeline that includes preprocessing and the model
5. Trains a Random Forest Regressor with 100 trees
6. Evaluates the model's performance
7. Saves the entire pipeline to a single file (model.pkl)

### Key Concepts:
- **Pipeline**: Chains preprocessing and model training into one object
- **ColumnTransformer**: Applies different preprocessing to different column types
- **SimpleImputer**: Fills in missing values automatically
- **OneHotEncoder**: Converts categorical text into numeric columns
- **Random Forest**: An ensemble of 100 decision trees that vote on predictions

### Why This Approach?
- Everything is in one pipeline, so preprocessing is automatically applied during prediction
- No need to manually preprocess data when deploying
- Prevents train-test skew (ensures training and production data are processed identically)

---

### COMPLETE CODE:

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import r2_score, mean_squared_error
import joblib

# 1. Load the Real Dataset
print("Downloading dataset from GitHub...")
url = "https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv"
df = pd.read_csv(url)

# 2. Separate Features and Target
X = df.drop("median_house_value", axis=1)
y = df["median_house_value"]

# 3. Define Features Types for Preprocessing
# We need to treat numeric and categorical (text) data differently
numeric_features = [
    "longitude", "latitude", "housing_median_age", "total_rooms",
    "total_bedrooms", "population", "households", "median_income"
]
categorical_features = ["ocean_proximity"]

# 4. Build the Preprocessing Pipeline
# Numeric: Fill missing values with the median
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median'))
    # No scaler here, as requested for Random Forest
])

# Categorical: Fill missing values with 'missing' and then OneHotEncode
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine them into a single ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# 5. Create the Full Pipeline (Preprocessor + Model)
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
])

# 6. Split Data
print("Splitting data...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 7. Train the Pipeline
print("Training the pipeline (preprocessing + model)...")
model_pipeline.fit(X_train, y_train)

# 8. Evaluate
print("Evaluating...")
predictions = model_pipeline.predict(X_test)
r2 = r2_score(y_test, predictions)
mse = mean_squared_error(y_test, predictions)

print(f"Model Training Complete!")
print(f"R2 Score: {r2:.4f}")
print(f"Mean Squared Error: {mse:.4f}")

# 9. Save the Pipeline
# We only need to save one file now. It contains everything.
joblib.dump(model_pipeline, 'model.pkl')
print("File 'model.pkl' saved successfully.")
```

### Step-by-Step Breakdown:

**Step 1-2: Data Loading**
- Downloads CSV file directly from GitHub repository
- Separates input features (X) from target variable (y - the house prices)

**Step 3: Feature Definition**
- Lists which columns are numeric (8 features)
- Lists which columns are categorical (1 feature: ocean_proximity)

**Step 4: Preprocessing Pipeline**
- Numeric pipeline: Fills missing values with median of that column
- Categorical pipeline: Fills missing values with "missing", then converts text to numbers
- ColumnTransformer: Applies the right preprocessing to the right columns

**Step 5: Complete Pipeline**
- Combines preprocessing with Random Forest model
- Everything happens in sequence automatically

**Step 6-7: Training**
- Splits data: 80% for training, 20% for testing
- Trains the entire pipeline on training data

**Step 8: Evaluation**
- Makes predictions on test data (data the model hasn't seen)
- Calculates R2 score (how well model explains variance) and MSE (average prediction error)

**Step 9: Save Model**
- Uses joblib to save the entire pipeline as model.pkl
- This file contains preprocessing steps AND the trained model

---
---

## FILE 3: app.py
## Purpose: Web Application (Flask Backend)

This Python script creates the web server that loads the trained model and handles user requests.

### What This File Does:
1. Imports necessary libraries (Flask for web server, pandas for data handling, joblib for loading model)
2. Creates a Flask application instance
3. Sets up logging to track what's happening
4. Loads the saved model.pkl file when the app starts
5. Defines routes (URL endpoints) that handle different requests
6. Processes form submissions and makes predictions
7. Returns results to the user

### Key Components:
- **Route "/"**: Displays the HTML form (home page)
- **Route "/predict"**: Receives form data, makes prediction, shows result
- **Route "/api/predict"**: Optional API endpoint for programmatic access
- **Error Handling**: Catches and displays errors gracefully

---

### COMPLETE CODE:

```python
import numpy as np
import pandas as pd
from flask import Flask, request, render_template, jsonify
import joblib
import logging

# Initialize Flask App
app = Flask(__name__)

# Configure Logging
logging.basicConfig(level=logging.INFO)
logger = app.logger

# Load the trained pipeline
try:
    logger.info("Loading model pipeline...")
    model = joblib.load('model.pkl')
    logger.info("Model loaded successfully.")
except Exception as e:
    logger.error(f"Error loading model: {e}")
    model = None

@app.route('/')
def home():
    """Renders the main page with the input form."""
    return render_template('index.html')

@app.route('/predict', methods=['POST'])
def predict():
    """
    Handles form submission, processes data, and returns prediction.
    """
    if not model:
        return render_template('index.html', error="Model not loaded. Please contact administrator.")

    try:
        # 1. Extract data from form
        # We must cast inputs to float, as HTML forms send strings
        form_data = request.form
        
        # 2. Create a DataFrame matching the training data structure exactly
        # The pipeline requires these specific column names
        input_data = pd.DataFrame({
            'longitude': [float(form_data['longitude'])],
            'latitude': [float(form_data['latitude'])],
            'housing_median_age': [float(form_data['housing_median_age'])],
            'total_rooms': [float(form_data['total_rooms'])],
            'total_bedrooms': [float(form_data['total_bedrooms'])],
            'population': [float(form_data['population'])],
            'households': [float(form_data['households'])],
            'median_income': [float(form_data['median_income'])],
            'ocean_proximity': [form_data['ocean_proximity']] # Categorical (string)
        })
        
        # 3. Make Prediction
        # The pipeline handles scaling and one-hot encoding automatically
        prediction = model.predict(input_data)[0]
        
        # 4. Return result
        return render_template(
            'index.html', 
            prediction_text=f"${prediction:,.2f}",
            input_data=form_data # Pass back input to keep form filled (optional)
        )

    except ValueError as ve:
        logger.error(f"Value Error: {ve}")
        return render_template('index.html', error="Invalid input. Please ensure all numeric fields contain numbers.")
    except Exception as e:
        logger.error(f"Prediction Error: {e}")
        return render_template('index.html', error=f"An error occurred: {str(e)}")

# API Endpoint (Optional, for programmatic access)
@app.route('/api/predict', methods=['POST'])
def predict_api():
    try:
        json_data = request.get_json()
        input_df = pd.DataFrame([json_data])
        prediction = model.predict(input_df)[0]
        return jsonify({'prediction': prediction})
    except Exception as e:
        return jsonify({'error': str(e)})

if __name__ == "__main__":
    app.run(debug=True)
```

### Detailed Explanation:

**Initialization Section:**
- Creates Flask application instance
- Sets up logging to track activities and errors
- Attempts to load model.pkl file (the trained pipeline from train_model.py)

**Route: / (Home Page)**
- Simply renders the index.html template
- This is what users see when they first visit the website

**Route: /predict (Main Prediction Handler)**
This is where the magic happens:

1. **Data Extraction**: Gets all form values from the POST request
2. **DataFrame Creation**: Converts form data into a pandas DataFrame with exact column names
   - CRITICAL: Column names must match training data exactly
   - Numeric values converted from strings to floats
   - Categorical value (ocean_proximity) kept as string
3. **Prediction**: Calls model.predict() which automatically:
   - Runs preprocessing (imputation, encoding)
   - Makes prediction using Random Forest
   - Returns a single number (predicted price)
4. **Response**: Renders index.html again with:
   - Prediction displayed in green box
   - Original form values preserved for user reference

**Error Handling:**
- ValueError: Catches invalid inputs (e.g., text in numeric fields)
- General Exception: Catches any other errors
- All errors displayed in red box on the form

**Route: /api/predict (Bonus API)**
- Accepts JSON data instead of form data
- Returns JSON response
- Useful for integrating with other applications

**Main Execution:**
- if __name__ == "__main__": ensures this only runs when script is executed directly
- debug=True provides helpful error messages during development

---
---

## HOW THE THREE FILES WORK TOGETHER

### The Complete Workflow:

**Phase 1: Training (One-Time Setup)**
1. Run train_model.py
2. Downloads data and trains model
3. Creates model.pkl file

**Phase 2: Deployment (Ongoing)**
1. Run app.py to start the web server
2. app.py loads model.pkl into memory
3. User visits website and sees index.html form
4. User fills form and clicks "Predict Price"
5. Form data sent to /predict endpoint in app.py
6. app.py converts form data to DataFrame
7. Model makes prediction using the pipeline
8. Result sent back and displayed in index.html
9. User sees predicted price in green box

### Critical Connections:

**Between train_model.py and app.py:**
- Column names defined in train_model.py MUST match DataFrame created in app.py
- Both use the same 9 features in the same order
- The pipeline saved by train_model.py is loaded by app.py

**Between app.py and index.html:**
- Form input names in HTML must match DataFrame column names in app.py
- Flask's render_template() passes data to HTML using Jinja2 syntax
- Prediction and error messages flow from app.py to display in HTML

**Model Pipeline Magic:**
- Because we saved a complete pipeline, we don't need to manually preprocess data in app.py
- Just create a DataFrame and call predict()
- The pipeline automatically handles missing values, encoding, and prediction

---
---

## TEACHING POINTS FOR TUTORIAL

### Key Concepts to Emphasize:

1. **End-to-End ML Project**: From raw data to deployed web application
2. **Pipeline Architecture**: Why bundling preprocessing with the model is powerful
3. **Column Name Consistency**: The most common source of errors in deployment
4. **Separation of Concerns**: Training (train_model.py) vs Serving (app.py)
5. **User Experience**: Form design, error handling, result display

### Common Mistakes to Warn About:

1. Mismatched column names between training and prediction
2. Forgetting to convert form strings to numeric types
3. Not handling missing values
4. Hardcoding values instead of using pipelines
5. Poor error messages that don't help users

### Extension Ideas for Learners:

1. Add more sophisticated CSS styling
2. Include data visualization of predictions
3. Add input validation on the frontend
4. Store predictions in a database
5. Add authentication for users
6. Deploy to cloud platforms (Heroku, AWS, etc.)

---

END OF DOCUMENTATION